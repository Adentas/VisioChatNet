{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvLnBJUP6a0M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization,Input\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Cifar10:\n",
        "    def __init__(self, train=True):\n",
        "        self.num_classes = 10\n",
        "        self.weight_decay = 0.0005\n",
        "        self.x_shape = [32,32,3]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        if train:\n",
        "            self.model = self.train(self.model)\n",
        "        else:\n",
        "            self.model.load_weights('cifar10vgg.h5')\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        # First block\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay), input_shape=self.x_shape))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Second block\n",
        "        model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Third block\n",
        "        model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Fourth block\n",
        "        model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        # Fifth block\n",
        "        model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        # Final dense layers\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, kernel_regularizer=l2(self.weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def normalize(self, X_train, X_test):\n",
        "        mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        X_train = (X_train - mean) / (std + 1e-7)\n",
        "        X_test = (X_test - mean) / (std + 1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        model_json = model.to_json()\n",
        "        with open('CIFAR10_model.json', 'w') as json_file:\n",
        "            json_file.write(model_json)\n",
        "        model.save_weights('CIFAR10_model.weights.h5')\n",
        "\n",
        "        model.save('CIFAR_10.hdf5')\n",
        "\n",
        "\n",
        "    def train(self, model):\n",
        "        batch_size = 128\n",
        "        maxepochs = 250\n",
        "        learning_rate = 0.1\n",
        "        lr_drop = 20\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "        y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "        datagen = ImageDataGenerator(\n",
        "          featurewise_center=False,\n",
        "          samplewise_center=False,\n",
        "          featurewise_std_normalization=False,\n",
        "          samplewise_std_normalization=False,\n",
        "          zca_whitening=False,\n",
        "          rotation_range=15,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          horizontal_flip=True,\n",
        "          vertical_flip=False)\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "                # EarlyStopping callback\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',  # Monitor validation loss\n",
        "            min_delta=0.001,     # Minimum change to qualify as an improvement\n",
        "            patience=15,         # Stop after 13 epochs with no improvement\n",
        "            verbose=1,           # Print messages\n",
        "            restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        "        )\n",
        "\n",
        "        sgd = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "              epochs=maxepochs, validation_data=(x_test, y_test),\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "\n",
        "        test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "        print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = Cifar10(train=True)\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_test, _ = model.normalize(x_test, x_test)  # Normalize test data\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, model.num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjc3UF_R6grK",
        "outputId": "2d41cb70-ec4f-4d31-8521-75acdec413fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 5s 0us/step\n",
            "Epoch 1/250\n",
            "391/391 [==============================] - 55s 109ms/step - loss: 8.7239 - accuracy: 0.2659 - val_loss: 6.3132 - val_accuracy: 0.2946 - lr: 0.1000\n",
            "Epoch 2/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 4.6757 - accuracy: 0.4366 - val_loss: 3.6324 - val_accuracy: 0.4597 - lr: 0.1000\n",
            "Epoch 3/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 2.8432 - accuracy: 0.5475 - val_loss: 2.4193 - val_accuracy: 0.5478 - lr: 0.1000\n",
            "Epoch 4/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.9525 - accuracy: 0.6266 - val_loss: 1.7899 - val_accuracy: 0.6154 - lr: 0.1000\n",
            "Epoch 5/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 1.5236 - accuracy: 0.6834 - val_loss: 1.4810 - val_accuracy: 0.6843 - lr: 0.1000\n",
            "Epoch 6/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.3690 - accuracy: 0.7154 - val_loss: 1.4162 - val_accuracy: 0.6810 - lr: 0.1000\n",
            "Epoch 7/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2784 - accuracy: 0.7362 - val_loss: 1.2044 - val_accuracy: 0.7620 - lr: 0.1000\n",
            "Epoch 8/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2538 - accuracy: 0.7541 - val_loss: 1.5422 - val_accuracy: 0.6730 - lr: 0.1000\n",
            "Epoch 9/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 1.2621 - accuracy: 0.7587 - val_loss: 1.2467 - val_accuracy: 0.7626 - lr: 0.1000\n",
            "Epoch 10/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2510 - accuracy: 0.7662 - val_loss: 1.2490 - val_accuracy: 0.7658 - lr: 0.1000\n",
            "Epoch 11/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.2435 - accuracy: 0.7748 - val_loss: 1.2783 - val_accuracy: 0.7664 - lr: 0.1000\n",
            "Epoch 12/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2453 - accuracy: 0.7784 - val_loss: 1.3057 - val_accuracy: 0.7544 - lr: 0.1000\n",
            "Epoch 13/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.2533 - accuracy: 0.7802 - val_loss: 1.3049 - val_accuracy: 0.7661 - lr: 0.1000\n",
            "Epoch 14/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2495 - accuracy: 0.7892 - val_loss: 1.2289 - val_accuracy: 0.7906 - lr: 0.1000\n",
            "Epoch 15/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 1.2453 - accuracy: 0.7923 - val_loss: 1.2389 - val_accuracy: 0.7889 - lr: 0.1000\n",
            "Epoch 16/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.2525 - accuracy: 0.7932 - val_loss: 1.2922 - val_accuracy: 0.7815 - lr: 0.1000\n",
            "Epoch 17/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2617 - accuracy: 0.7930 - val_loss: 1.2460 - val_accuracy: 0.7948 - lr: 0.1000\n",
            "Epoch 18/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.2544 - accuracy: 0.7951 - val_loss: 1.2689 - val_accuracy: 0.7864 - lr: 0.1000\n",
            "Epoch 19/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.2754 - accuracy: 0.7937 - val_loss: 1.2847 - val_accuracy: 0.7913 - lr: 0.1000\n",
            "Epoch 20/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 1.2838 - accuracy: 0.7944 - val_loss: 1.4925 - val_accuracy: 0.7404 - lr: 0.1000\n",
            "Epoch 21/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 1.1003 - accuracy: 0.8374 - val_loss: 1.0407 - val_accuracy: 0.8444 - lr: 0.0500\n",
            "Epoch 22/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.0037 - accuracy: 0.8450 - val_loss: 1.0394 - val_accuracy: 0.8285 - lr: 0.0500\n",
            "Epoch 23/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.9787 - accuracy: 0.8470 - val_loss: 0.9849 - val_accuracy: 0.8397 - lr: 0.0500\n",
            "Epoch 24/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.9787 - accuracy: 0.8452 - val_loss: 1.0123 - val_accuracy: 0.8351 - lr: 0.0500\n",
            "Epoch 25/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.9828 - accuracy: 0.8451 - val_loss: 1.0578 - val_accuracy: 0.8214 - lr: 0.0500\n",
            "Epoch 26/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.9785 - accuracy: 0.8489 - val_loss: 0.9662 - val_accuracy: 0.8515 - lr: 0.0500\n",
            "Epoch 27/250\n",
            "391/391 [==============================] - 40s 101ms/step - loss: 0.9834 - accuracy: 0.8499 - val_loss: 1.0757 - val_accuracy: 0.8244 - lr: 0.0500\n",
            "Epoch 28/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.9828 - accuracy: 0.8506 - val_loss: 0.9922 - val_accuracy: 0.8426 - lr: 0.0500\n",
            "Epoch 29/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.9931 - accuracy: 0.8512 - val_loss: 0.9750 - val_accuracy: 0.8579 - lr: 0.0500\n",
            "Epoch 30/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.9962 - accuracy: 0.8500 - val_loss: 1.0769 - val_accuracy: 0.8223 - lr: 0.0500\n",
            "Epoch 31/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 1.0002 - accuracy: 0.8516 - val_loss: 1.0681 - val_accuracy: 0.8272 - lr: 0.0500\n",
            "Epoch 32/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.9943 - accuracy: 0.8546 - val_loss: 1.0577 - val_accuracy: 0.8322 - lr: 0.0500\n",
            "Epoch 33/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 1.0033 - accuracy: 0.8543 - val_loss: 1.0014 - val_accuracy: 0.8536 - lr: 0.0500\n",
            "Epoch 34/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.9993 - accuracy: 0.8555 - val_loss: 0.9736 - val_accuracy: 0.8613 - lr: 0.0500\n",
            "Epoch 35/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.0070 - accuracy: 0.8541 - val_loss: 1.0433 - val_accuracy: 0.8384 - lr: 0.0500\n",
            "Epoch 36/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.0025 - accuracy: 0.8543 - val_loss: 1.1342 - val_accuracy: 0.8156 - lr: 0.0500\n",
            "Epoch 37/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.0027 - accuracy: 0.8569 - val_loss: 1.0433 - val_accuracy: 0.8426 - lr: 0.0500\n",
            "Epoch 38/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.0108 - accuracy: 0.8557 - val_loss: 0.9878 - val_accuracy: 0.8634 - lr: 0.0500\n",
            "Epoch 39/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.0161 - accuracy: 0.8563 - val_loss: 1.0164 - val_accuracy: 0.8555 - lr: 0.0500\n",
            "Epoch 40/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 1.0151 - accuracy: 0.8559 - val_loss: 1.1046 - val_accuracy: 0.8296 - lr: 0.0500\n",
            "Epoch 41/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.8908 - accuracy: 0.8890 - val_loss: 0.8678 - val_accuracy: 0.8881 - lr: 0.0250\n",
            "Epoch 42/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.8122 - accuracy: 0.9004 - val_loss: 0.8930 - val_accuracy: 0.8660 - lr: 0.0250\n",
            "Epoch 43/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.7805 - accuracy: 0.8997 - val_loss: 0.8289 - val_accuracy: 0.8805 - lr: 0.0250\n",
            "Epoch 44/250\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.7699 - accuracy: 0.9000 - val_loss: 0.8369 - val_accuracy: 0.8767 - lr: 0.0250\n",
            "Epoch 45/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.7575 - accuracy: 0.9000 - val_loss: 0.8286 - val_accuracy: 0.8759 - lr: 0.0250\n",
            "Epoch 46/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.7630 - accuracy: 0.8966 - val_loss: 0.8398 - val_accuracy: 0.8724 - lr: 0.0250\n",
            "Epoch 47/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.7544 - accuracy: 0.9000 - val_loss: 0.8849 - val_accuracy: 0.8549 - lr: 0.0250\n",
            "Epoch 48/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7600 - accuracy: 0.8982 - val_loss: 0.9020 - val_accuracy: 0.8607 - lr: 0.0250\n",
            "Epoch 49/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7593 - accuracy: 0.8998 - val_loss: 0.8518 - val_accuracy: 0.8719 - lr: 0.0250\n",
            "Epoch 50/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.7674 - accuracy: 0.8972 - val_loss: 0.8774 - val_accuracy: 0.8646 - lr: 0.0250\n",
            "Epoch 51/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.7694 - accuracy: 0.8980 - val_loss: 0.8384 - val_accuracy: 0.8778 - lr: 0.0250\n",
            "Epoch 52/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7635 - accuracy: 0.9023 - val_loss: 0.8300 - val_accuracy: 0.8816 - lr: 0.0250\n",
            "Epoch 53/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7649 - accuracy: 0.9022 - val_loss: 0.8033 - val_accuracy: 0.8913 - lr: 0.0250\n",
            "Epoch 54/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7605 - accuracy: 0.9036 - val_loss: 0.8638 - val_accuracy: 0.8714 - lr: 0.0250\n",
            "Epoch 55/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.7731 - accuracy: 0.8992 - val_loss: 0.8631 - val_accuracy: 0.8746 - lr: 0.0250\n",
            "Epoch 56/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.7718 - accuracy: 0.9033 - val_loss: 0.8552 - val_accuracy: 0.8795 - lr: 0.0250\n",
            "Epoch 57/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.7796 - accuracy: 0.9007 - val_loss: 0.8449 - val_accuracy: 0.8822 - lr: 0.0250\n",
            "Epoch 58/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.7750 - accuracy: 0.9020 - val_loss: 0.8896 - val_accuracy: 0.8684 - lr: 0.0250\n",
            "Epoch 59/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.7700 - accuracy: 0.9036 - val_loss: 0.8483 - val_accuracy: 0.8801 - lr: 0.0250\n",
            "Epoch 60/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.7719 - accuracy: 0.9043 - val_loss: 0.8538 - val_accuracy: 0.8824 - lr: 0.0250\n",
            "Epoch 61/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.6849 - accuracy: 0.9302 - val_loss: 0.7855 - val_accuracy: 0.9016 - lr: 0.0125\n",
            "Epoch 62/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.6305 - accuracy: 0.9405 - val_loss: 0.7235 - val_accuracy: 0.9074 - lr: 0.0125\n",
            "Epoch 63/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.6140 - accuracy: 0.9401 - val_loss: 0.7211 - val_accuracy: 0.9089 - lr: 0.0125\n",
            "Epoch 64/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.5949 - accuracy: 0.9416 - val_loss: 0.7521 - val_accuracy: 0.8957 - lr: 0.0125\n",
            "Epoch 65/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.5863 - accuracy: 0.9398 - val_loss: 0.7138 - val_accuracy: 0.9033 - lr: 0.0125\n",
            "Epoch 66/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5749 - accuracy: 0.9413 - val_loss: 0.7349 - val_accuracy: 0.8918 - lr: 0.0125\n",
            "Epoch 67/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.5742 - accuracy: 0.9388 - val_loss: 0.7195 - val_accuracy: 0.8969 - lr: 0.0125\n",
            "Epoch 68/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.5675 - accuracy: 0.9396 - val_loss: 0.7652 - val_accuracy: 0.8866 - lr: 0.0125\n",
            "Epoch 69/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.5659 - accuracy: 0.9395 - val_loss: 0.7188 - val_accuracy: 0.8961 - lr: 0.0125\n",
            "Epoch 70/250\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.5661 - accuracy: 0.9389 - val_loss: 0.7173 - val_accuracy: 0.8932 - lr: 0.0125\n",
            "Epoch 71/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.5654 - accuracy: 0.9396 - val_loss: 0.7138 - val_accuracy: 0.8961 - lr: 0.0125\n",
            "Epoch 72/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.5629 - accuracy: 0.9394 - val_loss: 0.7067 - val_accuracy: 0.8972 - lr: 0.0125\n",
            "Epoch 73/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5718 - accuracy: 0.9373 - val_loss: 0.7333 - val_accuracy: 0.8921 - lr: 0.0125\n",
            "Epoch 74/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.5674 - accuracy: 0.9382 - val_loss: 0.7355 - val_accuracy: 0.8894 - lr: 0.0125\n",
            "Epoch 75/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.5641 - accuracy: 0.9398 - val_loss: 0.7471 - val_accuracy: 0.8913 - lr: 0.0125\n",
            "Epoch 76/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5642 - accuracy: 0.9400 - val_loss: 0.7831 - val_accuracy: 0.8767 - lr: 0.0125\n",
            "Epoch 77/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.5666 - accuracy: 0.9391 - val_loss: 0.7209 - val_accuracy: 0.8962 - lr: 0.0125\n",
            "Epoch 78/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5667 - accuracy: 0.9397 - val_loss: 0.7360 - val_accuracy: 0.8917 - lr: 0.0125\n",
            "Epoch 79/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5685 - accuracy: 0.9387 - val_loss: 0.7624 - val_accuracy: 0.8898 - lr: 0.0125\n",
            "Epoch 80/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.5668 - accuracy: 0.9401 - val_loss: 0.7915 - val_accuracy: 0.8802 - lr: 0.0125\n",
            "Epoch 81/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.5051 - accuracy: 0.9599 - val_loss: 0.6739 - val_accuracy: 0.9116 - lr: 0.0063\n",
            "Epoch 82/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.4793 - accuracy: 0.9646 - val_loss: 0.6721 - val_accuracy: 0.9109 - lr: 0.0063\n",
            "Epoch 83/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4603 - accuracy: 0.9673 - val_loss: 0.6531 - val_accuracy: 0.9127 - lr: 0.0063\n",
            "Epoch 84/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4518 - accuracy: 0.9675 - val_loss: 0.6720 - val_accuracy: 0.9074 - lr: 0.0063\n",
            "Epoch 85/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4435 - accuracy: 0.9682 - val_loss: 0.6790 - val_accuracy: 0.9069 - lr: 0.0063\n",
            "Epoch 86/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.4361 - accuracy: 0.9690 - val_loss: 0.6697 - val_accuracy: 0.9054 - lr: 0.0063\n",
            "Epoch 87/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.4258 - accuracy: 0.9707 - val_loss: 0.6585 - val_accuracy: 0.9089 - lr: 0.0063\n",
            "Epoch 88/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.4260 - accuracy: 0.9680 - val_loss: 0.6462 - val_accuracy: 0.9084 - lr: 0.0063\n",
            "Epoch 89/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.4217 - accuracy: 0.9681 - val_loss: 0.6448 - val_accuracy: 0.9079 - lr: 0.0063\n",
            "Epoch 90/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.4144 - accuracy: 0.9685 - val_loss: 0.6375 - val_accuracy: 0.9121 - lr: 0.0063\n",
            "Epoch 91/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4087 - accuracy: 0.9697 - val_loss: 0.6295 - val_accuracy: 0.9111 - lr: 0.0063\n",
            "Epoch 92/250\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.4105 - accuracy: 0.9679 - val_loss: 0.6682 - val_accuracy: 0.9036 - lr: 0.0063\n",
            "Epoch 93/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 0.4071 - accuracy: 0.9680 - val_loss: 0.6620 - val_accuracy: 0.8997 - lr: 0.0063\n",
            "Epoch 94/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.4026 - accuracy: 0.9683 - val_loss: 0.6193 - val_accuracy: 0.9127 - lr: 0.0063\n",
            "Epoch 95/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4051 - accuracy: 0.9665 - val_loss: 0.6467 - val_accuracy: 0.9025 - lr: 0.0063\n",
            "Epoch 96/250\n",
            "391/391 [==============================] - 37s 96ms/step - loss: 0.4050 - accuracy: 0.9662 - val_loss: 0.6362 - val_accuracy: 0.9052 - lr: 0.0063\n",
            "Epoch 97/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.4008 - accuracy: 0.9676 - val_loss: 0.6757 - val_accuracy: 0.9018 - lr: 0.0063\n",
            "Epoch 98/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.3932 - accuracy: 0.9690 - val_loss: 0.6544 - val_accuracy: 0.9033 - lr: 0.0063\n",
            "Epoch 99/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.4010 - accuracy: 0.9657 - val_loss: 0.6449 - val_accuracy: 0.9028 - lr: 0.0063\n",
            "Epoch 100/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.3968 - accuracy: 0.9668 - val_loss: 0.6195 - val_accuracy: 0.9126 - lr: 0.0063\n",
            "Epoch 101/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.3658 - accuracy: 0.9769 - val_loss: 0.5951 - val_accuracy: 0.9190 - lr: 0.0031\n",
            "Epoch 102/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.3449 - accuracy: 0.9824 - val_loss: 0.5926 - val_accuracy: 0.9192 - lr: 0.0031\n",
            "Epoch 103/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.3354 - accuracy: 0.9841 - val_loss: 0.5842 - val_accuracy: 0.9226 - lr: 0.0031\n",
            "Epoch 104/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.3303 - accuracy: 0.9843 - val_loss: 0.5945 - val_accuracy: 0.9171 - lr: 0.0031\n",
            "Epoch 105/250\n",
            "391/391 [==============================] - 40s 102ms/step - loss: 0.3243 - accuracy: 0.9848 - val_loss: 0.5897 - val_accuracy: 0.9192 - lr: 0.0031\n",
            "Epoch 106/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.3194 - accuracy: 0.9850 - val_loss: 0.6000 - val_accuracy: 0.9161 - lr: 0.0031\n",
            "Epoch 107/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.3127 - accuracy: 0.9867 - val_loss: 0.5965 - val_accuracy: 0.9199 - lr: 0.0031\n",
            "Epoch 108/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.3123 - accuracy: 0.9844 - val_loss: 0.6197 - val_accuracy: 0.9133 - lr: 0.0031\n",
            "Epoch 109/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.3056 - accuracy: 0.9859 - val_loss: 0.5949 - val_accuracy: 0.9168 - lr: 0.0031\n",
            "Epoch 110/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.3047 - accuracy: 0.9857 - val_loss: 0.5805 - val_accuracy: 0.9191 - lr: 0.0031\n",
            "Epoch 111/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2987 - accuracy: 0.9857 - val_loss: 0.5942 - val_accuracy: 0.9142 - lr: 0.0031\n",
            "Epoch 112/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.3005 - accuracy: 0.9842 - val_loss: 0.6045 - val_accuracy: 0.9143 - lr: 0.0031\n",
            "Epoch 113/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2969 - accuracy: 0.9843 - val_loss: 0.5744 - val_accuracy: 0.9204 - lr: 0.0031\n",
            "Epoch 114/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2904 - accuracy: 0.9860 - val_loss: 0.5960 - val_accuracy: 0.9127 - lr: 0.0031\n",
            "Epoch 115/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.2909 - accuracy: 0.9847 - val_loss: 0.5653 - val_accuracy: 0.9173 - lr: 0.0031\n",
            "Epoch 116/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2865 - accuracy: 0.9855 - val_loss: 0.5680 - val_accuracy: 0.9185 - lr: 0.0031\n",
            "Epoch 117/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2869 - accuracy: 0.9847 - val_loss: 0.5579 - val_accuracy: 0.9217 - lr: 0.0031\n",
            "Epoch 118/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.2827 - accuracy: 0.9854 - val_loss: 0.5703 - val_accuracy: 0.9194 - lr: 0.0031\n",
            "Epoch 119/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2856 - accuracy: 0.9838 - val_loss: 0.5652 - val_accuracy: 0.9157 - lr: 0.0031\n",
            "Epoch 120/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2837 - accuracy: 0.9839 - val_loss: 0.5477 - val_accuracy: 0.9201 - lr: 0.0031\n",
            "Epoch 121/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2666 - accuracy: 0.9888 - val_loss: 0.5404 - val_accuracy: 0.9223 - lr: 0.0016\n",
            "Epoch 122/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2586 - accuracy: 0.9903 - val_loss: 0.5454 - val_accuracy: 0.9216 - lr: 0.0016\n",
            "Epoch 123/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.2527 - accuracy: 0.9919 - val_loss: 0.5479 - val_accuracy: 0.9221 - lr: 0.0016\n",
            "Epoch 124/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2486 - accuracy: 0.9927 - val_loss: 0.5537 - val_accuracy: 0.9209 - lr: 0.0016\n",
            "Epoch 125/250\n",
            "391/391 [==============================] - 38s 96ms/step - loss: 0.2458 - accuracy: 0.9926 - val_loss: 0.5637 - val_accuracy: 0.9215 - lr: 0.0016\n",
            "Epoch 126/250\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.2426 - accuracy: 0.9933 - val_loss: 0.5390 - val_accuracy: 0.9260 - lr: 0.0016\n",
            "Epoch 127/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2399 - accuracy: 0.9934 - val_loss: 0.5457 - val_accuracy: 0.9233 - lr: 0.0016\n",
            "Epoch 128/250\n",
            "391/391 [==============================] - 40s 101ms/step - loss: 0.2386 - accuracy: 0.9933 - val_loss: 0.5438 - val_accuracy: 0.9246 - lr: 0.0016\n",
            "Epoch 129/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2381 - accuracy: 0.9923 - val_loss: 0.5490 - val_accuracy: 0.9218 - lr: 0.0016\n",
            "Epoch 130/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2349 - accuracy: 0.9933 - val_loss: 0.5495 - val_accuracy: 0.9196 - lr: 0.0016\n",
            "Epoch 131/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2325 - accuracy: 0.9933 - val_loss: 0.5556 - val_accuracy: 0.9208 - lr: 0.0016\n",
            "Epoch 132/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2313 - accuracy: 0.9929 - val_loss: 0.5453 - val_accuracy: 0.9216 - lr: 0.0016\n",
            "Epoch 133/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2310 - accuracy: 0.9925 - val_loss: 0.5371 - val_accuracy: 0.9250 - lr: 0.0016\n",
            "Epoch 134/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2278 - accuracy: 0.9925 - val_loss: 0.5313 - val_accuracy: 0.9258 - lr: 0.0016\n",
            "Epoch 135/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2251 - accuracy: 0.9935 - val_loss: 0.5536 - val_accuracy: 0.9224 - lr: 0.0016\n",
            "Epoch 136/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 0.2226 - accuracy: 0.9936 - val_loss: 0.5482 - val_accuracy: 0.9200 - lr: 0.0016\n",
            "Epoch 137/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2190 - accuracy: 0.9937 - val_loss: 0.5382 - val_accuracy: 0.9236 - lr: 0.0016\n",
            "Epoch 138/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2176 - accuracy: 0.9936 - val_loss: 0.5431 - val_accuracy: 0.9238 - lr: 0.0016\n",
            "Epoch 139/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2162 - accuracy: 0.9937 - val_loss: 0.5569 - val_accuracy: 0.9196 - lr: 0.0016\n",
            "Epoch 140/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2140 - accuracy: 0.9941 - val_loss: 0.5480 - val_accuracy: 0.9205 - lr: 0.0016\n",
            "Epoch 141/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.2094 - accuracy: 0.9949 - val_loss: 0.5321 - val_accuracy: 0.9250 - lr: 7.8125e-04\n",
            "Epoch 142/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.2067 - accuracy: 0.9957 - val_loss: 0.5215 - val_accuracy: 0.9276 - lr: 7.8125e-04\n",
            "Epoch 143/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2041 - accuracy: 0.9961 - val_loss: 0.5178 - val_accuracy: 0.9303 - lr: 7.8125e-04\n",
            "Epoch 144/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 0.2029 - accuracy: 0.9961 - val_loss: 0.5356 - val_accuracy: 0.9256 - lr: 7.8125e-04\n",
            "Epoch 145/250\n",
            "391/391 [==============================] - 40s 101ms/step - loss: 0.2016 - accuracy: 0.9964 - val_loss: 0.5227 - val_accuracy: 0.9280 - lr: 7.8125e-04\n",
            "Epoch 146/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.2008 - accuracy: 0.9966 - val_loss: 0.5342 - val_accuracy: 0.9274 - lr: 7.8125e-04\n",
            "Epoch 147/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.1978 - accuracy: 0.9971 - val_loss: 0.5240 - val_accuracy: 0.9270 - lr: 7.8125e-04\n",
            "Epoch 148/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1968 - accuracy: 0.9968 - val_loss: 0.5297 - val_accuracy: 0.9258 - lr: 7.8125e-04\n",
            "Epoch 149/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 0.1965 - accuracy: 0.9965 - val_loss: 0.5259 - val_accuracy: 0.9284 - lr: 7.8125e-04\n",
            "Epoch 150/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1949 - accuracy: 0.9970 - val_loss: 0.5312 - val_accuracy: 0.9257 - lr: 7.8125e-04\n",
            "Epoch 151/250\n",
            "391/391 [==============================] - 40s 102ms/step - loss: 0.1938 - accuracy: 0.9968 - val_loss: 0.5167 - val_accuracy: 0.9285 - lr: 7.8125e-04\n",
            "Epoch 152/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1936 - accuracy: 0.9964 - val_loss: 0.5282 - val_accuracy: 0.9284 - lr: 7.8125e-04\n",
            "Epoch 153/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1928 - accuracy: 0.9964 - val_loss: 0.5273 - val_accuracy: 0.9296 - lr: 7.8125e-04\n",
            "Epoch 154/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1900 - accuracy: 0.9970 - val_loss: 0.5217 - val_accuracy: 0.9277 - lr: 7.8125e-04\n",
            "Epoch 155/250\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 0.1889 - accuracy: 0.9971 - val_loss: 0.5187 - val_accuracy: 0.9279 - lr: 7.8125e-04\n",
            "Epoch 156/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1873 - accuracy: 0.9975 - val_loss: 0.5270 - val_accuracy: 0.9268 - lr: 7.8125e-04\n",
            "Epoch 157/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1869 - accuracy: 0.9975 - val_loss: 0.5277 - val_accuracy: 0.9250 - lr: 7.8125e-04\n",
            "Epoch 158/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1853 - accuracy: 0.9974 - val_loss: 0.5265 - val_accuracy: 0.9274 - lr: 7.8125e-04\n",
            "Epoch 159/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1856 - accuracy: 0.9970 - val_loss: 0.5225 - val_accuracy: 0.9284 - lr: 7.8125e-04\n",
            "Epoch 160/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1838 - accuracy: 0.9972 - val_loss: 0.5235 - val_accuracy: 0.9242 - lr: 7.8125e-04\n",
            "Epoch 161/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1833 - accuracy: 0.9972 - val_loss: 0.5164 - val_accuracy: 0.9265 - lr: 3.9063e-04\n",
            "Epoch 162/250\n",
            "391/391 [==============================] - 39s 101ms/step - loss: 0.1804 - accuracy: 0.9981 - val_loss: 0.5150 - val_accuracy: 0.9265 - lr: 3.9063e-04\n",
            "Epoch 163/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1808 - accuracy: 0.9978 - val_loss: 0.5145 - val_accuracy: 0.9275 - lr: 3.9063e-04\n",
            "Epoch 164/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1786 - accuracy: 0.9984 - val_loss: 0.5183 - val_accuracy: 0.9277 - lr: 3.9063e-04\n",
            "Epoch 165/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1798 - accuracy: 0.9978 - val_loss: 0.5098 - val_accuracy: 0.9285 - lr: 3.9063e-04\n",
            "Epoch 166/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1782 - accuracy: 0.9979 - val_loss: 0.5146 - val_accuracy: 0.9282 - lr: 3.9063e-04\n",
            "Epoch 167/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1783 - accuracy: 0.9981 - val_loss: 0.5069 - val_accuracy: 0.9301 - lr: 3.9063e-04\n",
            "Epoch 168/250\n",
            "391/391 [==============================] - 40s 102ms/step - loss: 0.1772 - accuracy: 0.9980 - val_loss: 0.5162 - val_accuracy: 0.9280 - lr: 3.9063e-04\n",
            "Epoch 169/250\n",
            "391/391 [==============================] - 39s 101ms/step - loss: 0.1760 - accuracy: 0.9982 - val_loss: 0.5166 - val_accuracy: 0.9295 - lr: 3.9063e-04\n",
            "Epoch 170/250\n",
            "391/391 [==============================] - 39s 98ms/step - loss: 0.1764 - accuracy: 0.9980 - val_loss: 0.5195 - val_accuracy: 0.9274 - lr: 3.9063e-04\n",
            "Epoch 171/250\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1756 - accuracy: 0.9980 - val_loss: 0.5156 - val_accuracy: 0.9301 - lr: 3.9063e-04\n",
            "Epoch 172/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1742 - accuracy: 0.9986 - val_loss: 0.5194 - val_accuracy: 0.9304 - lr: 3.9063e-04\n",
            "Epoch 173/250\n",
            "391/391 [==============================] - 40s 102ms/step - loss: 0.1743 - accuracy: 0.9983 - val_loss: 0.5235 - val_accuracy: 0.9283 - lr: 3.9063e-04\n",
            "Epoch 174/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1733 - accuracy: 0.9983 - val_loss: 0.5161 - val_accuracy: 0.9299 - lr: 3.9063e-04\n",
            "Epoch 175/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1731 - accuracy: 0.9983 - val_loss: 0.5208 - val_accuracy: 0.9293 - lr: 3.9063e-04\n",
            "Epoch 176/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1719 - accuracy: 0.9986 - val_loss: 0.5160 - val_accuracy: 0.9283 - lr: 3.9063e-04\n",
            "Epoch 177/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1717 - accuracy: 0.9983 - val_loss: 0.5160 - val_accuracy: 0.9294 - lr: 3.9063e-04\n",
            "Epoch 178/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1719 - accuracy: 0.9984 - val_loss: 0.5150 - val_accuracy: 0.9305 - lr: 3.9063e-04\n",
            "Epoch 179/250\n",
            "391/391 [==============================] - 40s 101ms/step - loss: 0.1704 - accuracy: 0.9986 - val_loss: 0.5113 - val_accuracy: 0.9299 - lr: 3.9063e-04\n",
            "Epoch 180/250\n",
            "391/391 [==============================] - 39s 101ms/step - loss: 0.1700 - accuracy: 0.9984 - val_loss: 0.5138 - val_accuracy: 0.9295 - lr: 3.9063e-04\n",
            "Epoch 181/250\n",
            "391/391 [==============================] - 39s 99ms/step - loss: 0.1699 - accuracy: 0.9985 - val_loss: 0.5124 - val_accuracy: 0.9316 - lr: 1.9531e-04\n",
            "Epoch 182/250\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9983Restoring model weights from the end of the best epoch: 167.\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.1698 - accuracy: 0.9983 - val_loss: 0.5088 - val_accuracy: 0.9306 - lr: 1.9531e-04\n",
            "Epoch 182: early stopping\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.5069 - accuracy: 0.9301\n",
            "Test Accuracy: 93.01%\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 4, 4, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 2, 2, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 2, 2, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 1, 1, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9686602 (36.95 MB)\n",
            "Trainable params: 9679690 (36.93 MB)\n",
            "Non-trainable params: 6912 (27.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.save('CIFAR_10.hdf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phYul3F-KjMc",
        "outputId": "b504be1f-6fcd-4738-a97a-8ab01243e5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}